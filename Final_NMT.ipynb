{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decoder_tokens=17541\n",
    "num_encoder_tokens=14032 \n",
    "dimension=300\n",
    "max_length_eng,max_length_hindi=20,30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 300)    4209600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    5262300     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 300), (None, 721200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 300),  721200      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 17541)  5279841     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 16,194,141\n",
      "Trainable params: 16,194,141\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#encoder\n",
    "input_1=Input(shape=(None,))\n",
    "emb_1=Embedding(num_encoder_tokens,dimension,mask_zero=True)(input_1)\n",
    "_,state_h,state_c=LSTM(dimension,return_state=True)(emb_1)\n",
    "encoder_states=[state_h,state_c]\n",
    "#decoder\n",
    "input_2=Input(shape=(None,))\n",
    "emb_layer=Embedding(num_decoder_tokens,dimension,mask_zero=True)\n",
    "emb_2=emb_layer(input_2)\n",
    "lstm_layer=LSTM(dimension,return_sequences=True,return_state=True)\n",
    "lstm_out,_,_=lstm_layer(emb_2,initial_state=encoder_states)\n",
    "dense_layer=Dense(num_decoder_tokens,activation='sigmoid')\n",
    "out=dense_layer(lstm_out)\n",
    "model=Model(inputs=[input_1,input_2],outputs=out)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need For Inference\n",
    "while training ground truth is passed as input during each time step but while testing/predicting we don't have ground truth values so, output generated from previous time step is used as input to Decoder for next time step. Therefore, we generate each word of output sequence one by one which need inference mode to be set up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Algorithm \n",
    "- Predicting one word at a time.\n",
    "- After predicting a word the decoder states are preserved and are used as initial states for next time step.\n",
    "- At each time step, the predicted output is fed as input in the next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 300)         4209600   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 300), (None, 300) 721200    \n",
      "=================================================================\n",
      "Total params: 4,930,800\n",
      "Trainable params: 4,930,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#encoder\n",
    "encoder_model = Model(input_1, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    5262300     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 300),  721200      embedding_1[3][0]                \n",
      "                                                                 input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 17541)  5279841     lstm_1[3][0]                     \n",
      "==================================================================================================\n",
      "Total params: 11,263,341\n",
      "Trainable params: 11,263,341\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#decoder\n",
    "decoder_state_input_h = Input(shape=(dimension,)) \n",
    "decoder_state_input_c = Input(shape=(dimension,)) \n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_emb=emb_layer(input_2)\n",
    "decoder_outputs2, state_h2, state_c2=lstm_layer(decoder_emb,initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = dense_layer(decoder_outputs2) \n",
    "decoder_model = Model(inputs=[input_2] + decoder_states_inputs, outputs=[decoder_outputs2] + decoder_states2)\n",
    "# [input_2] + decoder_states_inputs, [decoder_outputs2] + decoder_states2 = List concatenation\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_to_idx_english.json','r') as f:\n",
    "    word_to_idx_english=json.load(f)\n",
    "with open('word_to_idx_hindi.json','r') as f:\n",
    "    word_to_idx_hindi=json.load(f)\n",
    "with open('idx_to_word_english.json','r') as f:\n",
    "    idx_to_word_english=json.load(f)\n",
    "with open('idx_to_word_hindi.json','r') as f:\n",
    "    idx_to_word_hindi=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eng(eng):\n",
    "    encoder_input=np.zeros((max_length_eng,),dtype='float32')    \n",
    "    eng=str(eng)\n",
    "    for idx,w in enumerate(eng.split()):\n",
    "        encoder_input[idx]=word_to_idx_english[w]\n",
    "    return encoder_input.reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('test_data.csv')\n",
    "l=len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder \n",
    "\n",
    "def decode_sequence(english_input):\n",
    "    current_state = encoder_model.predict(english_input) # Encode the input \n",
    "    seq = np.zeros((1,1))\n",
    "    seq[0,0]=word_to_idx_hindi['<start>']\n",
    "    output=''\n",
    "    is_complete=False\n",
    "    while not is_complete:\n",
    "        output_tokens, h, c=decoder_model.predict([seq]+current_state)\n",
    "        idx=np.argmax(output_tokens[0,-1,:])\n",
    "        new_word=idx_to_word_hindi[str(idx)]\n",
    "        output+=' '+new_word\n",
    "        if new_word=='<end>' or len(output.split())>=50:\n",
    "            is_complete=True\n",
    "        seq = np.zeros((1,1))\n",
    "        seq[0,0]=word_to_idx_hindi[new_word]\n",
    "        current_state=[h,c]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input -  to be able to communicate\n",
      "Given Translation -  हम बातचीते करें\n",
      "Predictions -  जो कि कैसे की\n"
     ]
    }
   ],
   "source": [
    "idx=np.random.randint(0,l)\n",
    "hindi_sent=decode_sequence(preprocess_eng(df.loc[idx]['english_sentence']))\n",
    "print('Input - ',df.loc[idx]['english_sentence'])\n",
    "given=' '.join(df.loc[idx]['hindi_sentence'].split()[1:-1])\n",
    "hindi_sent=' '.join(hindi_sent.split()[:-1])\n",
    "print('Given Translation - ',given)\n",
    "print('Predictions - ',hindi_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input -  which it was never designed for\n",
      "Given Translation -  जिसके लिए इसे कभी बनाया ही नहीं गया था\n",
      "Predictions -  जो कि वह ये था कि सिंधु लिपि\n"
     ]
    }
   ],
   "source": [
    "idx=np.random.randint(0,l)\n",
    "hindi_sent=decode_sequence(preprocess_eng(df.loc[idx]['english_sentence']))\n",
    "print('Input - ',df.loc[idx]['english_sentence'])\n",
    "given=' '.join(df.loc[idx]['hindi_sentence'].split()[1:-1])\n",
    "hindi_sent=' '.join(hindi_sent.split()[:-1])\n",
    "print('Given Translation - ',given)\n",
    "print('Predictions - ',hindi_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input -  and then are fit for ridicule\n",
      "Given Translation -  पर उपहास उड़ाने के लिए फिट ठीक है\n",
      "Predictions -  और फिर वो फिर भी\n"
     ]
    }
   ],
   "source": [
    "idx=np.random.randint(0,l)\n",
    "hindi_sent=decode_sequence(preprocess_eng(df.loc[idx]['english_sentence']))\n",
    "print('Input - ',df.loc[idx]['english_sentence'])\n",
    "given=' '.join(df.loc[idx]['hindi_sentence'].split()[1:-1])\n",
    "hindi_sent=' '.join(hindi_sent.split()[:-1])\n",
    "print('Given Translation - ',given)\n",
    "print('Predictions - ',hindi_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input -  love and pleasure\n",
      "Given Translation -  और आनंद चाहिए\n",
      "Predictions -  और फिर से नीचे\n"
     ]
    }
   ],
   "source": [
    "idx=np.random.randint(0,l)\n",
    "hindi_sent=decode_sequence(preprocess_eng(df.loc[idx]['english_sentence']))\n",
    "print('Input - ',df.loc[idx]['english_sentence'])\n",
    "given=' '.join(df.loc[idx]['hindi_sentence'].split()[1:-1])\n",
    "hindi_sent=' '.join(hindi_sent.split()[:-1])\n",
    "print('Given Translation - ',given)\n",
    "print('Predictions - ',hindi_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['dense_1', 'embedding_2', 'embedding_3', 'input_3', 'input_4', 'lstm_2', 'lstm_3']>\n",
      "['dense_1']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = \"model_weights.h5\"\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
